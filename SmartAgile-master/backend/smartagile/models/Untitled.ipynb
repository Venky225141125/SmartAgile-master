{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9cbef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90380ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Visual Studio</td>\n",
       "      <td>C:\\Program Files (x86)\\Microsoft Visual Studio...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eclipse</td>\n",
       "      <td>C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IntelliJ IDEA</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PyCharm</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WebStorm</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        app_name                                          file_path category\n",
       "0  Visual Studio  C:\\Program Files (x86)\\Microsoft Visual Studio...     work\n",
       "1        Eclipse  C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...     work\n",
       "2  IntelliJ IDEA  C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....     work\n",
       "3        PyCharm  C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...     work\n",
       "4       WebStorm  C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...     work"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"applications.csv\")\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a9b510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c0b0ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "072faceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"applications.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bd5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94261bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Visual Studio</td>\n",
       "      <td>C:\\Program Files (x86)\\Microsoft Visual Studio...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eclipse</td>\n",
       "      <td>C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IntelliJ IDEA</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PyCharm</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WebStorm</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        app_name                                          file_path category\n",
       "0  Visual Studio  C:\\Program Files (x86)\\Microsoft Visual Studio...     work\n",
       "1        Eclipse  C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...     work\n",
       "2  IntelliJ IDEA  C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....     work\n",
       "3        PyCharm  C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...     work\n",
       "4       WebStorm  C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...     work"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"applications.csv\")\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e992ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc1b96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.660377358490566\n",
      "Precision: 0.7096800656275636\n",
      "Recall: 0.660377358490566\n",
      "F1-score: 0.6032770605759682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('applications.csv')\n",
    "\n",
    "# Clean and preprocess the data\n",
    "df['app_name'] = df['app_name'].str.lower().str.strip()\n",
    "df['file_path'] = df['file_path'].str.lower().str.strip()\n",
    "\n",
    "# Extract additional features from the 'file_path' column\n",
    "df['dir_depth'] = df['file_path'].str.count(r'\\\\') + 1  # Adjusted for Windows paths\n",
    "df['domain'] = df['file_path'].str.split(r'\\\\').str[2]  # Assuming domain is the third part\n",
    "\n",
    "# Initialize CountVectorizer for each column\n",
    "vectorizer_app = CountVectorizer()\n",
    "vectorizer_path = CountVectorizer()\n",
    "\n",
    "# Fit and transform the 'app_name' and 'file_path' columns separately\n",
    "X_app = vectorizer_app.fit_transform(df['app_name'])\n",
    "X_path = vectorizer_path.fit_transform(df['file_path'])\n",
    "\n",
    "# Convert the matrices to arrays\n",
    "X_app_array = X_app.toarray()\n",
    "X_path_array = X_path.toarray()\n",
    "\n",
    "# Create DataFrames from the arrays\n",
    "df_app = pd.DataFrame(X_app_array, columns=vectorizer_app.get_feature_names_out())\n",
    "df_path = pd.DataFrame(X_path_array, columns=vectorizer_path.get_feature_names_out())\n",
    "\n",
    "# One-hot encode the 'domain' column\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "domain_encoded = encoder.fit_transform(df[['domain']])\n",
    "df_domain = pd.DataFrame(domain_encoded, columns=encoder.get_feature_names_out(['domain']))\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "X_combined = pd.concat([df_app, df_path, df[['dir_depth']].reset_index(drop=True), df_domain], axis=1)\n",
    "\n",
    "# Prepare the target variable\n",
    "y = df['category']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall:', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test, y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f82ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6037735849056604\n",
      "Precision: 0.6085868309587986\n",
      "Recall: 0.6037735849056604\n",
      "F1-score: 0.5093725476815996\n",
      "Predicted Category: entertainment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('applications.csv')\n",
    "\n",
    "# Clean and preprocess the data\n",
    "df['app_name'] = df['file_path'].str.lower().str.strip().str.split('\\\\').str[-1]\n",
    "# print(df['app_name'][:2])\n",
    "# Initialize CountVectorizer for 'app_name'\n",
    "vectorizer_app = CountVectorizer()\n",
    "\n",
    "# Fit and transform the 'app_name' column\n",
    "X_app = vectorizer_app.fit_transform(df['app_name'])\n",
    "\n",
    "# Convert the matrix to array\n",
    "X_app_array = X_app.toarray()\n",
    "\n",
    "# Create DataFrame from the array\n",
    "df_app = pd.DataFrame(X_app_array, columns=vectorizer_app.get_feature_names_out())\n",
    "\n",
    "# Prepare the target variable\n",
    "y = df['category']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_app, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# New application name\n",
    "new_app_name = [\"pdr.exe\"]\n",
    "\n",
    "# Transform the new app name using the fitted vectorizer\n",
    "new_app_vectorized = vectorizer_app.transform(new_app_name)\n",
    "\n",
    "# Convert the sparse matrix to array\n",
    "new_app_array = new_app_vectorized.toarray()\n",
    "\n",
    "# Predict the category\n",
    "predicted_category = model.predict(new_app_array)\n",
    "\n",
    "print(\"Predicted Category:\", predicted_category[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "407b2019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>devenv.exe</td>\n",
       "      <td>C:\\Program Files (x86)\\Microsoft Visual Studio...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eclipse.exe</td>\n",
       "      <td>C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idea64.exe</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pycharm64.exe</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>webstorm64.exe</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>icecat.exe</td>\n",
       "      <td>C:\\Program Files\\GNU IceCat\\icecat.exe</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>konqueror.exe</td>\n",
       "      <td>C:\\Program Files\\Konqueror\\konqueror.exe</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>navigator.exe</td>\n",
       "      <td>C:\\Program Files\\Netscape\\Navigator\\navigator.exe</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>duckduckgo.exe</td>\n",
       "      <td>C:\\Program Files\\DuckDuckGo Privacy Browser\\du...</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>iexplore.exe</td>\n",
       "      <td>C:\\Program Files\\Internet Explorer\\iexplore.exe</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           app_name                                          file_path  \\\n",
       "0        devenv.exe  C:\\Program Files (x86)\\Microsoft Visual Studio...   \n",
       "1       eclipse.exe  C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...   \n",
       "2        idea64.exe  C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....   \n",
       "3     pycharm64.exe  C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...   \n",
       "4    webstorm64.exe  C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...   \n",
       "..              ...                                                ...   \n",
       "521      icecat.exe             C:\\Program Files\\GNU IceCat\\icecat.exe   \n",
       "522   konqueror.exe           C:\\Program Files\\Konqueror\\konqueror.exe   \n",
       "523   navigator.exe  C:\\Program Files\\Netscape\\Navigator\\navigator.exe   \n",
       "524  duckduckgo.exe  C:\\Program Files\\DuckDuckGo Privacy Browser\\du...   \n",
       "525    iexplore.exe    C:\\Program Files\\Internet Explorer\\iexplore.exe   \n",
       "\n",
       "     category  \n",
       "0        work  \n",
       "1        work  \n",
       "2        work  \n",
       "3        work  \n",
       "4        work  \n",
       "..        ...  \n",
       "521  personal  \n",
       "522  personal  \n",
       "523  personal  \n",
       "524  personal  \n",
       "525  personal  \n",
       "\n",
       "[526 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e66d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2878b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617c2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a8bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a part of your browser's title (e.g., 'Google Chrome'): google\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "import pyperclip\n",
    "\n",
    "# Function to bring the browser to focus by its title\n",
    "def focus_browser_window(title):\n",
    "    # Locate the center of the window with the specified title\n",
    "    window = pyautogui.getWindowsWithTitle(title)\n",
    "    if window:\n",
    "        window[0].activate()\n",
    "        time.sleep(0.3)  # Adjusted wait time\n",
    "\n",
    "# Function to simulate 'Ctrl+L' and 'Ctrl+C' to copy the current URL\n",
    "def copy_url_from_browser():\n",
    "    # Select the URL in the address bar\n",
    "    pyautogui.hotkey('ctrl', 'l')\n",
    "    time.sleep(0.2)  # Adjusted wait time\n",
    "\n",
    "    # Copy the URL to clipboard\n",
    "    pyautogui.hotkey('ctrl', 'c')\n",
    "    time.sleep(0.2)  # Adjusted wait time\n",
    "\n",
    "    # Return the copied URL\n",
    "    return pyperclip.paste()\n",
    "\n",
    "# Function to switch to the next tab in most browsers using 'Ctrl+Tab'\n",
    "def switch_to_next_tab():\n",
    "    pyautogui.hotkey('ctrl', 'tab')\n",
    "    time.sleep(0.3)  # Adjusted wait time\n",
    "\n",
    "# Main logic\n",
    "if __name__ == \"__main__\":\n",
    "    browser_title = input(\"Enter a part of your browser's title (e.g., 'Google Chrome'): \")\n",
    "    number_of_tabs = int(input(\"Enter the number of tabs open in your browser: \"))\n",
    "    \n",
    "    focus_browser_window(browser_title)\n",
    "    \n",
    "    try:\n",
    "        for _ in range(number_of_tabs):\n",
    "            url = copy_url_from_browser()\n",
    "            print(f\"URL of current tab: {url}\")\n",
    "            switch_to_next_tab()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nMonitoring stopped by user.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd1f497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a part of your browser's title (e.g., 'Google Chrome'): google\n",
      "New tab detected: Untitled - Jupyter Notebook - Google Chrome\n",
      "URL copied: http://localhost:8888/notebooks/Untitled.ipynb\n",
      "New tab detected: Home - Google Chrome\n",
      "URL copied: https://chatgpt.com/c/439991ba-2412-4fb4-8174-b79b713d2d84\n",
      "New tab detected: ChatGPT - Google Chrome\n",
      "New tab detected: Untitled - Jupyter Notebook - Google Chrome\n",
      "New tab detected: Home - Google Chrome\n",
      "New tab detected: ChatGPT - Google Chrome\n",
      "New tab detected: New Tab - Google Chrome\n",
      "New tab detected: mail.google.com/mail/u/0/?tab=rm&ogbl - Google Chrome\n",
      "URL copied: https://mail.google.com/mail/u/0/?tab=rm&ogbl\n",
      "New tab detected: Untitled - Jupyter Notebook - Google Chrome\n",
      "New tab detected: Inbox (89) - tarunutla1234@gmail.com - Gmail - Google Chrome\n",
      "New tab detected: Untitled - Jupyter Notebook - Google Chrome\n",
      "Monitoring stopped by user.\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "import pyperclip\n",
    "import pygetwindow as gw\n",
    "import win32gui\n",
    "\n",
    "# Function to list all open windows and print their titles\n",
    "def list_open_windows():\n",
    "    windows = gw.getAllTitles()\n",
    "    for window in windows:\n",
    "        if window.strip():  # Skip empty titles\n",
    "            print(window)\n",
    "\n",
    "# Function to bring the browser to focus by its title\n",
    "def focus_browser_window(title):\n",
    "    windows = [w for w in gw.getWindowsWithTitle(title) if title.lower() in w.title.lower()]\n",
    "    if windows:\n",
    "        windows[0].activate()\n",
    "        time.sleep(1)  # Wait for the browser to come into focus\n",
    "    else:\n",
    "        print(f\"No window found with title containing: {title}\")\n",
    "\n",
    "# Function to simulate 'Ctrl+L' and 'Ctrl+C' to copy the current URL\n",
    "def copy_url_from_browser():\n",
    "    pyautogui.hotkey('ctrl', 'l')\n",
    "    time.sleep(0.1)  # Wait for the address bar to be selected\n",
    "    pyautogui.hotkey('ctrl', 'c')\n",
    "    time.sleep(0.1)  # Wait for the copy command to complete\n",
    "    return pyperclip.paste()\n",
    "\n",
    "# Function to monitor for new tabs\n",
    "def monitor_tabs(browser_title, check_interval=2):\n",
    "    copied_urls = set()  # Set to store copied URLs\n",
    "    current_windows = set()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get a list of windows that match the browser title\n",
    "            windows = [w for w in gw.getWindowsWithTitle(browser_title) if browser_title.lower() in w.title.lower()]\n",
    "            if not windows:\n",
    "                print(f\"No window found with title containing: {browser_title}\")\n",
    "                list_open_windows()  # List all open windows for debugging\n",
    "                time.sleep(check_interval)\n",
    "                continue\n",
    "\n",
    "            # Get current window titles\n",
    "            new_windows = {w.title for w in windows}\n",
    "            new_tabs = new_windows - current_windows\n",
    "\n",
    "            for tab in new_tabs:\n",
    "                print(f\"New tab detected: {tab}\")\n",
    "                focus_browser_window(tab)\n",
    "                new_url = copy_url_from_browser()\n",
    "\n",
    "                if new_url not in copied_urls:\n",
    "                    copied_urls.add(new_url)\n",
    "                    print(f\"URL copied: {new_url}\")\n",
    "\n",
    "            current_windows = new_windows\n",
    "            time.sleep(check_interval)  # Delay before checking again\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Monitoring stopped by user.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "# Main logic\n",
    "if __name__ == \"__main__\":\n",
    "    browser_title = input(\"Enter a part of your browser's title (e.g., 'Google Chrome'): \")\n",
    "    if not browser_title.strip():\n",
    "        print(\"You must enter a part of the browser's title to continue.\")\n",
    "    else:\n",
    "        monitor_tabs(browser_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c57e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51153c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Windows\n",
      "[nltk_data]     10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Windows\n",
      "[nltk_data]     10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Windows\n",
      "[nltk_data]     10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65f8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import joblib\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6223f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (replace with your actual dataset)\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Windows 10\\\\Downloads\\\\SmartAgile-main-main\\\\SmartAgile\\\\backend\\\\smartagile\\\\models\\\\browsertasks.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f26c071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(691)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034c17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data cleaning (assuming 'keyword' and 'category' are the relevant columns)\n",
    "df.dropna(subset=['keyword', 'category'], inplace=True)  # Drop rows with NaN values\n",
    "df.drop_duplicates(subset=['keyword'], inplace=True)  # Drop duplicate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10fff75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4305"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4422584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'svm__C': 1, 'svm__gamma': 'scale', 'tfidf__max_features': None}\n",
      "\n",
      "Accuracy: 0.8614551083591331\n",
      "\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "   Educational-related       0.92      0.52      0.66        66\n",
      " Entertainment-related       0.67      0.65      0.66        62\n",
      "              Personal       0.97      0.99      0.98        85\n",
      "   Educational-related       0.82      0.91      0.86       284\n",
      " Entertainment related       0.92      0.86      0.89       177\n",
      " Entertainment-related       0.90      0.78      0.84       194\n",
      "       Finance-related       0.88      0.79      0.83        56\n",
      "        Gaming-related       0.82      0.94      0.87        33\n",
      "                Health       0.74      1.00      0.85        20\n",
      "              Personal       0.00      0.00      0.00         1\n",
      "          Work-related       0.87      0.95      0.90       314\n",
      "\n",
      "              accuracy                           0.86      1292\n",
      "             macro avg       0.77      0.76      0.76      1292\n",
      "          weighted avg       0.86      0.86      0.86      1292\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 34   0   0  25   0   0   1   0   1   0   5]\n",
      " [  0  40   1   0   6   5   0   0   0   0  10]\n",
      " [  0   0  84   0   0   0   0   0   0   0   1]\n",
      " [  3   1   1 258   3   3   2   0   0   0  13]\n",
      " [  0   8   0   3 153   7   0   1   0   0   5]\n",
      " [  0  11   1  12   3 152   3   6   2   0   4]\n",
      " [  0   0   0   5   1   0  44   0   0   0   6]\n",
      " [  0   0   0   0   0   1   0  31   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0  20   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1]\n",
      " [  0   0   0  13   0   0   0   0   4   0 297]]\n",
      "\n",
      "Predictions for new data:\n",
      "New project planning techniques --> Work-related\n",
      "video songs --> Work-related\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Windows 10\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))  # Expand this list based on your dataset's context\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Removing stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Removing numbers (if they're not relevant)\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "    # Removing extra whitespace\n",
    "    text = \" \".join(tokens).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['clean_keyword'] = df['keyword'].apply(preprocess_text)\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer()  # Removed max_features to let feature selection happen in GridSearchCV\n",
    "\n",
    "# Initialize SVM classifier within a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('svm', SVC(kernel='linear', random_state=42))  # Linear kernel is chosen for simplicity\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_keyword'], df['category'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV with expanded parameter grid and cross-validation\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "    'tfidf__max_features': [500, 1000, None]  # None means all features are used\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set using the best pipeline\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model with additional metrics\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix and Error Analysis\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "\n",
    "# Save the trained pipeline (includes both vectorizer and model)\n",
    "joblib.dump(best_pipeline, 'svm_pipeline.pkl')\n",
    "\n",
    "# Load the saved pipeline\n",
    "loaded_pipeline = joblib.load('svm_pipeline.pkl')\n",
    "\n",
    "# Example: Predicting new data using the loaded pipeline\n",
    "new_data = [\"New project planning techniques\", \"video songs\"]\n",
    "predictions = loaded_pipeline.predict(new_data)\n",
    "print(\"\\nPredictions for new data:\")\n",
    "for text, prediction in zip(new_data, predictions):\n",
    "    print(f\"{text} --> {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00cc8c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Entertainment-related'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_new = vectorizer.transform()\n",
    "loaded_pipeline.predict([\"The Prelude Of Kalki 2898 AD - Episode 1 [ENGLISH] Reaction | Nag Ashwin | #Kalki2898ADonJune27 - YouTube - Google Chrome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7da0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8aa784ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'svm__C': 1, 'svm__gamma': 'scale', 'tfidf__max_features': None}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m X_new \u001b[38;5;241m=\u001b[39m loaded_pipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(X_new_preprocessed)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Predict using the loaded SVM model\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Print predictions\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPredictions for new data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\pipeline.py:602\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 602\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:2162\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2146\u001b[0m \n\u001b[0;32m   2147\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2160\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2162\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1434\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1434\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1436\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import joblib\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import enchant  # Import pyenchant\n",
    "\n",
    "# Sample data (replace with your actual dataset)\n",
    "df = pd.read_csv(\"smartagile/browsertasks.csv\")\n",
    "\n",
    "# Data cleaning (assuming 'keyword' and 'category' are the relevant columns)\n",
    "df.dropna(subset=['keyword', 'category'], inplace=True)  # Drop rows with NaN values\n",
    "df.drop_duplicates(subset=['keyword'], inplace=True)  # Drop duplicate keywords\n",
    "\n",
    "# Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))  # Expand this list based on your dataset's context\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Initialize the English dictionary from pyenchant\n",
    "english_dict = enchant.Dict(\"en_US\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if not english_dict.check(word):\n",
    "            suggestions = english_dict.suggest(word)\n",
    "            if suggestions:\n",
    "                corrected_words.append(suggestions[0])  # Use the first suggestion as correction\n",
    "            else:\n",
    "                corrected_words.append(word)  # If no suggestions, keep the original word\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "df['clean_keyword'] = df['keyword'].apply(preprocess_text)\n",
    "df['clean_keyword'] = df['clean_keyword'].apply(correct_spelling)\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer()  # Removed max_features to let feature selection happen in GridSearchCV\n",
    "\n",
    "# Initialize SVM classifier within a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('svm', SVC(kernel='linear', random_state=42))  # Linear kernel is chosen for simplicity\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_keyword'], df['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV with expanded parameter grid and cross-validation\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "    'tfidf__max_features': [500, 1000, None]  # None means all features are used\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Save the trained pipeline (includes both vectorizer and model)\n",
    "joblib.dump(best_pipeline, 'svm_pipeline.pkl')\n",
    "\n",
    "# Load the saved pipeline\n",
    "loaded_pipeline = joblib.load('svm_pipeline.pkl')\n",
    "\n",
    "# Example: Predicting new data using the loaded pipeline\n",
    "new_data = [\"ChatGPT - Google Chrome\", \"Healthcare administration skills\"]\n",
    "\n",
    "# Preprocess new data\n",
    "X_new_preprocessed = pd.Series(new_data).apply(preprocess_text)\n",
    "\n",
    "# Transform the preprocessed data using the loaded vectorizer\n",
    "X_new = loaded_pipeline.named_steps['tfidf'].transform(X_new_preprocessed)\n",
    "\n",
    "# Predict using the loaded SVM model\n",
    "predictions = loaded_pipeline.predict(X_new)\n",
    "\n",
    "# Print predictions\n",
    "print(\"\\nPredictions for new data:\")\n",
    "for text, prediction in zip(new_data, predictions):\n",
    "    print(f\"{text} --> {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc4beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
