{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9cbef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90380ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Visual Studio</td>\n",
       "      <td>C:\\Program Files (x86)\\Microsoft Visual Studio...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eclipse</td>\n",
       "      <td>C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IntelliJ IDEA</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PyCharm</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WebStorm</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        app_name                                          file_path category\n",
       "0  Visual Studio  C:\\Program Files (x86)\\Microsoft Visual Studio...     work\n",
       "1        Eclipse  C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...     work\n",
       "2  IntelliJ IDEA  C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....     work\n",
       "3        PyCharm  C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...     work\n",
       "4       WebStorm  C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...     work"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"applications.csv\")\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a9b510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c0b0ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "072faceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"applications.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bd5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94261bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Visual Studio</td>\n",
       "      <td>C:\\Program Files (x86)\\Microsoft Visual Studio...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eclipse</td>\n",
       "      <td>C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IntelliJ IDEA</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PyCharm</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WebStorm</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        app_name                                          file_path category\n",
       "0  Visual Studio  C:\\Program Files (x86)\\Microsoft Visual Studio...     work\n",
       "1        Eclipse  C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...     work\n",
       "2  IntelliJ IDEA  C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....     work\n",
       "3        PyCharm  C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...     work\n",
       "4       WebStorm  C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...     work"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"applications.csv\")\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e992ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc1b96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.660377358490566\n",
      "Precision: 0.7096800656275636\n",
      "Recall: 0.660377358490566\n",
      "F1-score: 0.6032770605759682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('applications.csv')\n",
    "\n",
    "# Clean and preprocess the data\n",
    "df['app_name'] = df['app_name'].str.lower().str.strip()\n",
    "df['file_path'] = df['file_path'].str.lower().str.strip()\n",
    "\n",
    "# Extract additional features from the 'file_path' column\n",
    "df['dir_depth'] = df['file_path'].str.count(r'\\\\') + 1  # Adjusted for Windows paths\n",
    "df['domain'] = df['file_path'].str.split(r'\\\\').str[2]  # Assuming domain is the third part\n",
    "\n",
    "# Initialize CountVectorizer for each column\n",
    "vectorizer_app = CountVectorizer()\n",
    "vectorizer_path = CountVectorizer()\n",
    "\n",
    "# Fit and transform the 'app_name' and 'file_path' columns separately\n",
    "X_app = vectorizer_app.fit_transform(df['app_name'])\n",
    "X_path = vectorizer_path.fit_transform(df['file_path'])\n",
    "\n",
    "# Convert the matrices to arrays\n",
    "X_app_array = X_app.toarray()\n",
    "X_path_array = X_path.toarray()\n",
    "\n",
    "# Create DataFrames from the arrays\n",
    "df_app = pd.DataFrame(X_app_array, columns=vectorizer_app.get_feature_names_out())\n",
    "df_path = pd.DataFrame(X_path_array, columns=vectorizer_path.get_feature_names_out())\n",
    "\n",
    "# One-hot encode the 'domain' column\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "domain_encoded = encoder.fit_transform(df[['domain']])\n",
    "df_domain = pd.DataFrame(domain_encoded, columns=encoder.get_feature_names_out(['domain']))\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "X_combined = pd.concat([df_app, df_path, df[['dir_depth']].reset_index(drop=True), df_domain], axis=1)\n",
    "\n",
    "# Prepare the target variable\n",
    "y = df['category']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall:', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test, y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f82ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6037735849056604\n",
      "Precision: 0.6085868309587986\n",
      "Recall: 0.6037735849056604\n",
      "F1-score: 0.5093725476815996\n",
      "Predicted Category: entertainment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('applications.csv')\n",
    "\n",
    "# Clean and preprocess the data\n",
    "df['app_name'] = df['file_path'].str.lower().str.strip().str.split('\\\\').str[-1]\n",
    "# print(df['app_name'][:2])\n",
    "# Initialize CountVectorizer for 'app_name'\n",
    "vectorizer_app = CountVectorizer()\n",
    "\n",
    "# Fit and transform the 'app_name' column\n",
    "X_app = vectorizer_app.fit_transform(df['app_name'])\n",
    "\n",
    "# Convert the matrix to array\n",
    "X_app_array = X_app.toarray()\n",
    "\n",
    "# Create DataFrame from the array\n",
    "df_app = pd.DataFrame(X_app_array, columns=vectorizer_app.get_feature_names_out())\n",
    "\n",
    "# Prepare the target variable\n",
    "y = df['category']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_app, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# New application name\n",
    "new_app_name = [\"pdr.exe\"]\n",
    "\n",
    "# Transform the new app name using the fitted vectorizer\n",
    "new_app_vectorized = vectorizer_app.transform(new_app_name)\n",
    "\n",
    "# Convert the sparse matrix to array\n",
    "new_app_array = new_app_vectorized.toarray()\n",
    "\n",
    "# Predict the category\n",
    "predicted_category = model.predict(new_app_array)\n",
    "\n",
    "print(\"Predicted Category:\", predicted_category[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "407b2019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>devenv.exe</td>\n",
       "      <td>C:\\Program Files (x86)\\Microsoft Visual Studio...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eclipse.exe</td>\n",
       "      <td>C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idea64.exe</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pycharm64.exe</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>webstorm64.exe</td>\n",
       "      <td>C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>icecat.exe</td>\n",
       "      <td>C:\\Program Files\\GNU IceCat\\icecat.exe</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>konqueror.exe</td>\n",
       "      <td>C:\\Program Files\\Konqueror\\konqueror.exe</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>navigator.exe</td>\n",
       "      <td>C:\\Program Files\\Netscape\\Navigator\\navigator.exe</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>duckduckgo.exe</td>\n",
       "      <td>C:\\Program Files\\DuckDuckGo Privacy Browser\\du...</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>iexplore.exe</td>\n",
       "      <td>C:\\Program Files\\Internet Explorer\\iexplore.exe</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           app_name                                          file_path  \\\n",
       "0        devenv.exe  C:\\Program Files (x86)\\Microsoft Visual Studio...   \n",
       "1       eclipse.exe  C:\\Users\\username\\eclipse\\java-2021-06\\eclipse...   \n",
       "2        idea64.exe  C:\\Program Files\\JetBrains\\IntelliJ IDEA 2021....   \n",
       "3     pycharm64.exe  C:\\Program Files\\JetBrains\\PyCharm 2021.1.1\\bi...   \n",
       "4    webstorm64.exe  C:\\Program Files\\JetBrains\\WebStorm 2021.1.1\\b...   \n",
       "..              ...                                                ...   \n",
       "521      icecat.exe             C:\\Program Files\\GNU IceCat\\icecat.exe   \n",
       "522   konqueror.exe           C:\\Program Files\\Konqueror\\konqueror.exe   \n",
       "523   navigator.exe  C:\\Program Files\\Netscape\\Navigator\\navigator.exe   \n",
       "524  duckduckgo.exe  C:\\Program Files\\DuckDuckGo Privacy Browser\\du...   \n",
       "525    iexplore.exe    C:\\Program Files\\Internet Explorer\\iexplore.exe   \n",
       "\n",
       "     category  \n",
       "0        work  \n",
       "1        work  \n",
       "2        work  \n",
       "3        work  \n",
       "4        work  \n",
       "..        ...  \n",
       "521  personal  \n",
       "522  personal  \n",
       "523  personal  \n",
       "524  personal  \n",
       "525  personal  \n",
       "\n",
       "[526 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e66d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2878b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617c2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a8bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a part of your browser's title (e.g., 'Google Chrome'): google\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "import pyperclip\n",
    "\n",
    "# Function to bring the browser to focus by its title\n",
    "def focus_browser_window(title):\n",
    "    # Locate the center of the window with the specified title\n",
    "    window = pyautogui.getWindowsWithTitle(title)\n",
    "    if window:\n",
    "        window[0].activate()\n",
    "        time.sleep(0.3)  # Adjusted wait time\n",
    "\n",
    "# Function to simulate 'Ctrl+L' and 'Ctrl+C' to copy the current URL\n",
    "def copy_url_from_browser():\n",
    "    # Select the URL in the address bar\n",
    "    pyautogui.hotkey('ctrl', 'l')\n",
    "    time.sleep(0.2)  # Adjusted wait time\n",
    "\n",
    "    # Copy the URL to clipboard\n",
    "    pyautogui.hotkey('ctrl', 'c')\n",
    "    time.sleep(0.2)  # Adjusted wait time\n",
    "\n",
    "    # Return the copied URL\n",
    "    return pyperclip.paste()\n",
    "\n",
    "# Function to switch to the next tab in most browsers using 'Ctrl+Tab'\n",
    "def switch_to_next_tab():\n",
    "    pyautogui.hotkey('ctrl', 'tab')\n",
    "    time.sleep(0.3)  # Adjusted wait time\n",
    "\n",
    "# Main logic\n",
    "if __name__ == \"__main__\":\n",
    "    browser_title = input(\"Enter a part of your browser's title (e.g., 'Google Chrome'): \")\n",
    "    number_of_tabs = int(input(\"Enter the number of tabs open in your browser: \"))\n",
    "    \n",
    "    focus_browser_window(browser_title)\n",
    "    \n",
    "    try:\n",
    "        for _ in range(number_of_tabs):\n",
    "            url = copy_url_from_browser()\n",
    "            print(f\"URL of current tab: {url}\")\n",
    "            switch_to_next_tab()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nMonitoring stopped by user.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd1f497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a part of your browser's title (e.g., 'Google Chrome'): google\n",
      "New tab detected: Untitled - Jupyter Notebook - Google Chrome\n",
      "URL copied: http://localhost:8888/notebooks/Untitled.ipynb\n",
      "New tab detected: Home - Google Chrome\n",
      "URL copied: https://chatgpt.com/c/439991ba-2412-4fb4-8174-b79b713d2d84\n",
      "New tab detected: ChatGPT - Google Chrome\n",
      "New tab detected: Untitled - Jupyter Notebook - Google Chrome\n",
      "New tab detected: Home - Google Chrome\n",
      "New tab detected: ChatGPT - Google Chrome\n",
      "New tab detected: New Tab - Google Chrome\n",
      "New tab detected: mail.google.com/mail/u/0/?tab=rm&ogbl - Google Chrome\n",
      "URL copied: https://mail.google.com/mail/u/0/?tab=rm&ogbl\n",
      "New tab detected: Untitled - Jupyter Notebook - Google Chrome\n",
      "New tab detected: Inbox (89) - tarunutla1234@gmail.com - Gmail - Google Chrome\n",
      "New tab detected: Untitled - Jupyter Notebook - Google Chrome\n",
      "Monitoring stopped by user.\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "import pyperclip\n",
    "import pygetwindow as gw\n",
    "import win32gui\n",
    "\n",
    "# Function to list all open windows and print their titles\n",
    "def list_open_windows():\n",
    "    windows = gw.getAllTitles()\n",
    "    for window in windows:\n",
    "        if window.strip():  # Skip empty titles\n",
    "            print(window)\n",
    "\n",
    "# Function to bring the browser to focus by its title\n",
    "def focus_browser_window(title):\n",
    "    windows = [w for w in gw.getWindowsWithTitle(title) if title.lower() in w.title.lower()]\n",
    "    if windows:\n",
    "        windows[0].activate()\n",
    "        time.sleep(1)  # Wait for the browser to come into focus\n",
    "    else:\n",
    "        print(f\"No window found with title containing: {title}\")\n",
    "\n",
    "# Function to simulate 'Ctrl+L' and 'Ctrl+C' to copy the current URL\n",
    "def copy_url_from_browser():\n",
    "    pyautogui.hotkey('ctrl', 'l')\n",
    "    time.sleep(0.1)  # Wait for the address bar to be selected\n",
    "    pyautogui.hotkey('ctrl', 'c')\n",
    "    time.sleep(0.1)  # Wait for the copy command to complete\n",
    "    return pyperclip.paste()\n",
    "\n",
    "# Function to monitor for new tabs\n",
    "def monitor_tabs(browser_title, check_interval=2):\n",
    "    copied_urls = set()  # Set to store copied URLs\n",
    "    current_windows = set()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get a list of windows that match the browser title\n",
    "            windows = [w for w in gw.getWindowsWithTitle(browser_title) if browser_title.lower() in w.title.lower()]\n",
    "            if not windows:\n",
    "                print(f\"No window found with title containing: {browser_title}\")\n",
    "                list_open_windows()  # List all open windows for debugging\n",
    "                time.sleep(check_interval)\n",
    "                continue\n",
    "\n",
    "            # Get current window titles\n",
    "            new_windows = {w.title for w in windows}\n",
    "            new_tabs = new_windows - current_windows\n",
    "\n",
    "            for tab in new_tabs:\n",
    "                print(f\"New tab detected: {tab}\")\n",
    "                focus_browser_window(tab)\n",
    "                new_url = copy_url_from_browser()\n",
    "\n",
    "                if new_url not in copied_urls:\n",
    "                    copied_urls.add(new_url)\n",
    "                    print(f\"URL copied: {new_url}\")\n",
    "\n",
    "            current_windows = new_windows\n",
    "            time.sleep(check_interval)  # Delay before checking again\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Monitoring stopped by user.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "# Main logic\n",
    "if __name__ == \"__main__\":\n",
    "    browser_title = input(\"Enter a part of your browser's title (e.g., 'Google Chrome'): \")\n",
    "    if not browser_title.strip():\n",
    "        print(\"You must enter a part of the browser's title to continue.\")\n",
    "    else:\n",
    "        monitor_tabs(browser_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c57e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d51153c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d65f8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import joblib\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6223f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (replace with your actual dataset)\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\tmachine\\\\smartAgile\\\\browsertasks.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f26c071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(691)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "034c17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data cleaning (assuming 'keyword' and 'category' are the relevant columns)\n",
    "df.dropna(subset=['keyword', 'category'], inplace=True)  # Drop rows with NaN values\n",
    "df.drop_duplicates(subset=['keyword'], inplace=True)  # Drop duplicate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a10fff75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5030"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4422584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))  # Expand this list based on your dataset's context\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Removing stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Removing numbers (if they're not relevant)\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "    # Removing extra whitespace\n",
    "    text = \" \".join(tokens).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "309606b6-88db-45ed-af6f-b7da3c594d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'svm__C': 1, 'svm__gamma': 'scale', 'tfidf__max_features': None, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "Accuracy: 0.8601722995361166\n",
      "\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "   Educational-related       0.88      0.57      0.69        67\n",
      " Entertainment-related       0.72      0.64      0.68        59\n",
      "              Personal       0.95      0.99      0.97        92\n",
      "     Education-related       0.95      0.49      0.65        43\n",
      "   Educational-related       0.80      0.89      0.84       264\n",
      " Entertainment related       0.91      0.86      0.88       181\n",
      " Entertainment-related       0.87      0.90      0.88       373\n",
      "       Finance-related       0.89      0.80      0.84        60\n",
      "        Gaming-related       0.87      0.89      0.88        37\n",
      "                Health       0.83      0.94      0.88        16\n",
      "              Personal       0.00      0.00      0.00         1\n",
      "          Work-related       0.86      0.91      0.89       316\n",
      "\n",
      "              accuracy                           0.86      1509\n",
      "             macro avg       0.79      0.74      0.76      1509\n",
      "          weighted avg       0.86      0.86      0.86      1509\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 38   0   0   0  20   0   2   1   0   1   0   5]\n",
      " [  0  38   1   0   0   5  14   0   0   0   0   1]\n",
      " [  0   0  91   0   0   0   0   0   0   0   0   1]\n",
      " [  2   0   0  21  11   0   3   0   0   0   0   6]\n",
      " [  1   1   0   0 236   4   7   2   1   0   0  12]\n",
      " [  0   3   0   0   3 155  16   0   1   0   0   3]\n",
      " [  0  11   3   0   7   6 334   1   3   1   0   7]\n",
      " [  0   0   0   0   1   0   2  48   0   0   0   9]\n",
      " [  0   0   0   0   1   0   2   0  33   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0  15   0   1]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0]\n",
      " [  2   0   1   1  17   0   3   2   0   1   0 289]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for new data:\n",
      "New project planning techniques --> Work-related\n",
      "video songs --> Entertainment-related\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['clean_keyword'] = df['keyword'].apply(preprocess_text)\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer()  # Removed max_features to let feature selection happen in GridSearchCV\n",
    "\n",
    "# Initialize SVM classifier within a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('svm', SVC(kernel='linear', random_state=42))  # Linear kernel is chosen for simplicity\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_keyword'], df['category'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV with expanded parameter grid and cross-validation\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "    'tfidf__max_features': [500, 1000, None],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)]# None means all features are used\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set using the best pipeline\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model with additional metrics\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix and Error Analysis\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "\n",
    "# Save the trained pipeline (includes both vectorizer and model)\n",
    "joblib.dump(best_pipeline, 'svm_pipeline.pkl')\n",
    "\n",
    "# Load the saved pipeline\n",
    "loaded_pipeline = joblib.load('svm_pipeline.pkl')\n",
    "\n",
    "# Example: Predicting new data using the loaded pipeline\n",
    "new_data = [\"New project planning techniques\", \"video songs\"]\n",
    "predictions = loaded_pipeline.predict(new_data)\n",
    "print(\"\\nPredictions for new data:\")\n",
    "for text, prediction in zip(new_data, predictions):\n",
    "    print(f\"{text} --> {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00cc8c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Entertainment-related'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_new = vectorizer.transform()\n",
    "loaded_pipeline = joblib.load('svm_pipeline.pkl')\n",
    "loaded_pipeline.predict([\"Dhanush's Jabardasth Telugu Comedy Back 2 Back Comedy Scenes || Latest Telugu Comedy 2016 - YouTube - Brave\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b7da0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Task\n",
      "0  Dhanush's Jabardasth Telugu Comedy Back 2 Back...\n"
     ]
    }
   ],
   "source": [
    "task=\"Dhanush's Jabardasth Telugu Comedy Back 2 Back Comedy Scenes || Latest Telugu Comedy 2016 - YouTube - Brave\"\n",
    "prediction_input = pd.DataFrame([task],columns=['Task'])\n",
    "print(prediction_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8aa784ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Work-related'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pipeline.predict(prediction_input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5dc4beb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords, wordnet\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\compat.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[0;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CupyOps,\n\u001b[0;32m      3\u001b[0m     MPSOps,\n\u001b[0;32m      4\u001b[0m     NumpyOps,\n\u001b[0;32m      5\u001b[0m     Ops,\n\u001b[0;32m      6\u001b[0m     get_current_ops,\n\u001b[0;32m      7\u001b[0m     get_ops,\n\u001b[0;32m      8\u001b[0m     set_current_ops,\n\u001b[0;32m      9\u001b[0m     set_gpu_allocator,\n\u001b[0;32m     10\u001b[0m     use_ops,\n\u001b[0;32m     11\u001b[0m     use_pytorch_for_gpu_memory,\n\u001b[0;32m     12\u001b[0m     use_tensorflow_for_gpu_memory,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\backends\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cupy_allocators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\backends\\cupy_ops.py:16\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     is_cupy_array,\n\u001b[0;32m      8\u001b[0m     is_mxnet_gpu_array,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     torch2xp,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39mops(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCupyOps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCupyOps\u001b[39;00m(Ops):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\thinc\\backends\\numpy_ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.backends.numpy_ops\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Ensure necessary downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize NLP tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\tmachine\\\\smartAgile\\\\browsertasks.csv\")\n",
    "\n",
    "# Adding custom stop words based on context\n",
    "additional_stopwords = {'project', 'planning', 'techniques'}\n",
    "stop_words.update(additional_stopwords)\n",
    "\n",
    "def handle_negations(text):\n",
    "    if sid.polarity_scores(text)['neg'] > 0.5:\n",
    "        return 'negated ' + text\n",
    "    return text\n",
    "\n",
    "def spacy_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "def stem_and_lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens]\n",
    "\n",
    "def advanced_preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = handle_negations(text)  # Handle negations\n",
    "    tokens = spacy_tokenize(text)  # Advanced tokenization\n",
    "    tokens = stem_and_lemmatize(tokens)  # Stemming and lemmatization\n",
    "    tokens = [token for token in tokens if token not in stop_words and not token.isdigit()]  # Remove stop words and digits\n",
    "    text = \" \".join(tokens).strip()  # Join tokens into a clean string\n",
    "    return text\n",
    "\n",
    "# Apply advanced preprocessing\n",
    "df['clean_keyword'] = df['keyword'].apply(advanced_preprocess_text)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_keyword'], df['category'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the TF-IDF vectorizer and the SVM classifier within a pipeline\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('svm', SVC(kernel='linear', random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100, 1000],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "    'tfidf__max_features': [500, 1000, 5000, None],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)]\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and model\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# Save the trained model pipeline\n",
    "joblib.dump(best_pipeline, 'enhanced_svm_pipeline.pkl')\n",
    "\n",
    "# Load the saved model pipeline\n",
    "loaded_pipeline = joblib.load('enhanced_svm_pipeline.pkl')\n",
    "\n",
    "# Predicting new data using the loaded pipeline\n",
    "new_data = [\"New project planning techniques\", \"Enjoyable video songs\"]\n",
    "predictions = loaded_pipeline.predict(new_data)\n",
    "print(\"\\nPredictions for new data:\")\n",
    "for text, prediction in zip(new_data, predictions):\n",
    "    print(f\"{text} --> {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4c18e68-2db5-46eb-9e7c-7fff620728f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting spacy\n",
      "  Downloading spacy-3.7.5-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.20.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp310-cp310-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.7.5-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/12.1 MB 10.2 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.5/12.1 MB 5.6 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.5/12.1 MB 5.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.4/12.1 MB 6.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 6.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.0/12.1 MB 6.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.5/12.1 MB 6.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.5/12.1 MB 6.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.8/12.1 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/12.1 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.0/12.1 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.1 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.6/12.1 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 6.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.3/12.1 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.7/12.1 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.0/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.3/12.1 MB 6.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.4/12.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.7/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.3/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.6/12.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.8/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.4/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.0/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.1 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.1/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.6/12.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.1 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.1 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 5.6 MB/s eta 0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.2/122.2 kB ? eta 0:00:00\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Downloading pydantic_core-2.20.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.9 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.9 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.9/1.9 MB 6.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.9 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.9 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 6.0 MB/s eta 0:00:00\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 481.9/481.9 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 15.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 11.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 7.2 MB/s eta 0:00:00\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/6.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.7/6.6 MB 7.3 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.0/6.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.3/6.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.7/6.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.0/6.6 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.0/6.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.3/6.6 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.6/6.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.8/6.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.2/6.6 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.6/6.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.2/6.6 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.6 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.8/6.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.0/6.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 5.7 MB/s eta 0:00:00\n",
      "Using cached cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.0-cp310-cp310-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.4/152.4 kB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, pydantic-core, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.8.2 pydantic-core-2.20.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b921a3-461b-434f-883f-64be32012c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
